# Training Guide for VL-FOG: Visionâ€“Language Flow Gating Framework for Early Accident Anticipation - [TAISC 2025](https://sites.google.com/view/avss2025-tw/taisc)


## ğŸ“Š Performance

|              | accuracy | f1 score | AUC    | Score  |
|--------------|----------|----------|--------|--------|
| Training  | 0.7746   | 0.7573   | 0.7790 | 0.7668 |
| Private test | 0.6992   | 0.6262   | 0.7314 | 0.6691 |



## ğŸš€ Quick Start & Workflow for Training

### 1. **Install Dependencies**

```bash
pip install -r requirements.txt
```

### 2. **Prepare Data**
- Download the [offical AVA dataset](https://drive.google.com/file/d/1F1Vaat_ZrpITtDtlo4WB5cwUHEfMYon2/view)
- Place your training CSVs and frame folders as specified in your `config/training_config_stepLR.yaml` ([see project structure](#project-structure)).
```
data/AVA_Dataset/
â”œâ”€â”€ freeway_train.csv
â”œâ”€â”€ road_train.csv
â”œâ”€â”€ freeway/train/<video_folder>/00001.jpg ...
â””â”€â”€ road/train/<video_folder>/00001.jpg ...
```

### 3. **Download YOLO Weights**
- Download the [YOLO Weights](https://drive.google.com/file/d/1wC4Wxtlg7bv4W7gbrsHd2lcYh59Ow9me/view?usp=sharing) (e.g., `yolo11x_trained.pt`).

Or run command:
```bash
gdown 1wC4Wxtlg7bv4W7gbrsHd2lcYh59Ow9me
```
- Place the weights file in your project directory or a path of your choice.
- In `utils/extract_bb.py`, set the `YOLO_WEIGHTS` variable to the path of your weights file:
  ```python
  YOLO_WEIGHTS = "yolo11x_trained.pt"
  ```

### 4. **Extract Vehicle Bounding Boxes**

- Run the bounding box extraction script for the desired split:
  ```bash
  python utils/extract_bb.py --split train # For training
  ```
- This will use YOLO to detect vehicles in all frames and cache the results.
- **Bounding boxes are saved as compressed pickle files** in `.bb_cache` folders inside each video directory (e.g., `data/AVA_Dataset/freeway/train/.bb_cache/`).
- You only need to run this step once per dataset (unless you change the frames or YOLO weights).

### 5. **Train the Model**:

**Note:** Training on different devices may yield different results, even with the same configuration. Additionally, the results are also affected by the object detection performance from YOLO. The results presented here were obtained under optimal conditions during our best training runs.

- You can use the provided shell script for training:
  ```bash
  chmod +x train.sh
  ```
  ```bash
  ./train.sh
  ```
- Or run manually:
  ```bash
  python main.py --config config/training_config_stepLR.yaml
  ```

Training logs, best model, and TensorBoard logs will be saved in a new `runs/YYYYMMDD_HHMMSS/` directory (the latest directory).

### 6. **Monitor Training**

- Use the provided script to launch TensorBoard:
  ```bash
  python launch_board.py
  ```
- This will automatically find the latest run and open TensorBoard at the correct log directory.
- You can also specify a custom log directory with `--runs_dir`.

### 7. **Evaluate/Test & Generate Prediction File**

- **Model Weight:** Download the [Pretrained Model](https://drive.google.com/file/d/1mfk1D4iDenlneQmqPAYGWWmit6xO8xKs/view?usp=sharing) and place it at the path specified in your config (e.g., `models/best_model.pth`).  

Or run command:
```bash
gdown 1mfk1D4iDenlneQmqPAYGWWmit6xO8xKs
```
- **Important:** Before running `test.py`, set the correct `model_path`, the dataset path (`frame_root`), and the `output_csv` file in your config file (e.g., `config/training_config_stepLR.yaml`):
  ```yaml
  test:
    model_path: "models/best_model.pth"
    frame_root: 
      freeway: data/AVA_Dataset/freeway/test
      road: data/AVA_Dataset/road/test
    output_csv: "results/test_predictions.csv"
    ...
  ```
- Then run:
  ```bash
  python test.py
  ```
- The prediction file will be saved to the path specified in your config.

## ğŸ–¥ï¸ Device Used for Training

All experiments and benchmarks in this project were conducted on:

- **GPU:** NVIDIA RTX A6000 (49GB VRAM)


## ğŸ§ª Data Splitting: train_df.csv and val_df.csv

After extensive experiments with **k-fold cross-validation (k=5)**, we found that a particular split of the data led to the best model performance. As a result, we use `data/train_df.csv` and `data/val_df.csv` as the fixed training and validation sets for all experiments. This split consistently yields better results than random or stratified splits.

- `data/train_df.csv`: CSV listing the best training set (file_name, risk, etc.)
- `data/val_df.csv`: CSV listing the best validation set

If you want to reproduce the best results, use these files for training and validation.

---

<details>
<summary id="project-structure">Project Structure</summary>

```
TAISC-Challenge/
â”œâ”€â”€ main.py                # Main training script
â”œâ”€â”€ test.py                # Model evaluation script
â”œâ”€â”€ launch_board.py        # Launch TensorBoard easily
â”œâ”€â”€ train.sh               # Shell script for training
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ training_config_stepLR.yaml # Main configuration file
â”‚   â””â”€â”€ training_config_plateau.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_loader.py     # Data loading utilities
â”‚   â”œâ”€â”€ dataset.py         # Dataset class with caching
â”‚   â”œâ”€â”€ train_df.csv       # Best training split
â”‚   â”œâ”€â”€ val_df.csv         # Best validation split 
â”‚   â””â”€â”€ augmentation.py    # Data augmentation
â”‚   â””â”€â”€ AVA_Dataset/       # <--- Folder containing video frame folders 
â”‚       â”œâ”€â”€ freeway_train.csv
â”‚       â”œâ”€â”€ road_train.csv
â”‚       â”œâ”€â”€ freeway/
â”‚       â”‚   â””â”€â”€ train/
â”‚       â”‚       â””â”€â”€ <video_folder>/
â”‚       â”‚           â”œâ”€â”€ 00001.jpg
â”‚       â”‚           â”œâ”€â”€ 00002.jpg
â”‚       â”‚           â””â”€â”€ ...
â”‚       â””â”€â”€ road/
â”‚           â””â”€â”€ train/
â”‚               â””â”€â”€ <video_folder>/
â”‚                   â”œâ”€â”€ 00001.jpg
â”‚                   â”œâ”€â”€ 00002.jpg
â”‚                   â””â”€â”€ ...
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_loader.py    # Model loading logic
â”‚   â””â”€â”€ clip_fusion_net.py # Model architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py          # Config loading
â”‚   â”œâ”€â”€ setup.py           # Training setup utilities
â”‚   â”œâ”€â”€ log.py             # Logging helpers
â”‚   â”œâ”€â”€ training.py        # Training loop and helpers
â”‚   â”œâ”€â”€ board.py           # TensorBoard utilities
â”‚   â”œâ”€â”€ seed.py            # Random seed setup
â”‚   â”œâ”€â”€ extract_bb.py      # YOLO vehicle detection
â”‚   â””â”€â”€ optical_flow.py    # Optical flow feature extraction
â”œâ”€â”€ runs/                  # Training outputs (created automatically)
â”‚   â””â”€â”€ YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ best_model.pth
â”‚       â”œâ”€â”€ log.txt
â”‚       â””â”€â”€ tensorboard/
â””â”€â”€ requirements.txt       # Python dependencies
```

</details>

<details>
<summary>Script Overview</summary>

### main.py
- Loads config and sets up directories, logging, and TensorBoard
- Initializes model, optimizer, loss, scheduler, and data loaders
- Runs the training loop with early stopping and best model saving
- Logs all progress and metrics
- Supports custom config via `--config` argument

### test.py
- Loads the best model and runs inference on the test set
- Saves predictions to CSV

### utils/extract_bb.py
- Uses YOLO to detect vehicles in video frames
- Caches bounding box data for efficient processing
- Supports multiprocessing for faster extraction
- **Bounding boxes are saved in `.bb_cache` folders inside each video directory.**

### utils/optical_flow.py
- Computes optical flow features using detected vehicle bounding boxes
- Implements caching for performance optimization
- Provides both basic and bounding box-aware flow extraction

### launch_board.py
- Use this script to launch TensorBoard for the latest run or a custom log directory.
- Example:
  ```bash
  python launch_board.py --latest
  ```

</details>

---

## ğŸ”— References

- [Ultralytics YOLO Documentation](https://docs.ultralytics.com/)
- [CLIP Model](https://github.com/openai/CLIP)
- [TAISC 2025](https://sites.google.com/view/avss2025-tw/taisc)


## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.