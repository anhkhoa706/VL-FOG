# VL-FOG: Visionâ€“Language Flow Gating Framework for Early Accident Anticipation - [TAISC 2025](https://sites.google.com/view/avss2025-tw/taisc)

This repository contains the main training pipeline for the **VL-FOG** model, designed for the TAISC (Taiwan Accident Classification) Challenge. The model predicts imminent traffic accidents from dash-cam video using a vision-language approach with vehicle detection via YOLO.



| #   | Participant | Entries | Date              | Score   | Accuracy | F1 Score | AUC  |
|-----|-------------|---------|-------------------|---------|----------|----------|------|
| ðŸ¥‡  | **kane110 (Our team)** | **1**     | **2025-07-11 02:25**  | **0.67** | **0.70**     | **0.63**     | **0.73** |
| ðŸ¥ˆ  | qyming      | 1       | 2025-07-12 22:43  | **0.66** | 0.72     | 0.60     | 0.71 |
| ðŸ¥‰  | hsu_lab     | 1       | 2025-07-13 06:49  | **0.65** | 0.69     | 0.60     | 0.71 |


## Table of Contents

| Guide            | Description                                  |
|------------------|----------------------------------------------|
| Test Guide | Generate Submission File (see below)         |
| [Training Guide](training_guide.md)                | Full training instructions (external guide)  |

---

## ðŸš€ Quick Start for Make Submission File

### 1. **Install Dependencies**

```bash
pip install -r requirements.txt
```

### 2. **Prepare Data**
- Download the [offical AVA dataset](https://drive.google.com/file/d/1F1Vaat_ZrpITtDtlo4WB5cwUHEfMYon2/view)
- Place your training CSVs and frame folders as specified in your `config/training_config_stepLR.yaml` ([see project structure](#project-structure)).

```
data/AVA_Dataset/
â”œâ”€â”€ freeway_train.csv
â”œâ”€â”€ road_train.csv
â”œâ”€â”€ freeway/train/<video_folder>/00001.jpg ...
â””â”€â”€ road/train/<video_folder>/00001.jpg ...
```

### 3. **Download YOLO Weights**
- Download the [YOLO Weights](https://drive.google.com/file/d/1wC4Wxtlg7bv4W7gbrsHd2lcYh59Ow9me/view?usp=sharing) (e.g., `yolo11x_trained.pt`).

Or run command
```bash
gdown 1wC4Wxtlg7bv4W7gbrsHd2lcYh59Ow9me
```
- Place the weights file in your project directory or a path of your choice.
- In `utils/extract_bb.py`, set the `YOLO_WEIGHTS` variable to the path of your weights file:
  ```python
  YOLO_WEIGHTS = "yolo11x_trained.pt"
  ```

### 4. **Extract Vehicle Bounding Boxes**

- Run the bounding box extraction script for the desired split:
  ```bash
  python utils/extract_bb.py --split test  # For testing - generate submission file
  ```
- This will use YOLO to detect vehicles in all frames and cache the results.
- **Bounding boxes are saved as compressed pickle files** in `.bb_cache` folders inside each video directory (e.g., `data/AVA_Dataset/freeway/test/.bb_cache/`).
- You only need to run this step once per dataset (unless you change the frames or YOLO weights).

### 5. **Edit Config** & **Generate Prediction File**

- **Model Weight:** Download the [Pretrained Model](https://drive.google.com/file/d/1mfk1D4iDenlneQmqPAYGWWmit6xO8xKs/view?usp=sharing) and place it at the path specified in your config (e.g., `models/best_model.pth`).  

```bash
gdown 1mfk1D4iDenlneQmqPAYGWWmit6xO8xKs
```
- **Important:** Before running `test.py`, set the correct `model_path`, the dataset path (`frame_root`), and the `output_csv` file in your config file (e.g., `config/training_config_stepLR.yaml`):
  ```yaml
  test:
    model_path: "models/best_model.pth"
    frame_root: 
      freeway: data/AVA_Dataset/freeway/test
      road: data/AVA_Dataset/road/test
    output_csv: "results/test_predictions.csv"
  ```- Then run:
  ```bash
  python test.py
  ```
- The prediction file will be saved to the path specified in your config.


## **Note** 
Training on different devices may yield different results, even with the same configuration. Additionally, the results are also affected by the object detection performance from YOLO. The results presented here were obtained under optimal conditions during our best training runs.


## ðŸ§ª Data Splitting: train_df.csv and val_df.csv

After extensive experiments with **k-fold cross-validation (k=5)**, we found that a particular split of the data led to the best model performance. As a result, we use `data/train_df.csv` and `data/val_df.csv` as the fixed training and validation sets for all experiments. This split consistently yields better results than random or stratified splits.

- `data/train_df.csv`: CSV listing the best training set (file_name, risk, etc.)
- `data/val_df.csv`: CSV listing the best validation set

If you want to reproduce the better results, use these files for training and validation.

---

<details>
<summary id="project-structure">Project Structure</summary>

```
TAISC-Challenge/
â”œâ”€â”€ main.py                # Main training script
â”œâ”€â”€ test.py                # Model evaluation script
â”œâ”€â”€ launch_board.py        # Launch TensorBoard easily
â”œâ”€â”€ train.sh               # Shell script for training
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ training_config_stepLR.yaml # Main configuration file
â”‚   â””â”€â”€ training_config_plateau.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_loader.py     # Data loading utilities
â”‚   â”œâ”€â”€ dataset.py         # Dataset class with caching
â”‚   â”œâ”€â”€ train_df.csv       # Best training split
â”‚   â”œâ”€â”€ val_df.csv         # Best validation split 
â”‚   â””â”€â”€ augmentation.py    # Data augmentation
â”‚   â””â”€â”€ AVA_Dataset/       # <--- Folder containing video frame folders 
â”‚       â”œâ”€â”€ freeway_train.csv
â”‚       â”œâ”€â”€ road_train.csv
â”‚       â”œâ”€â”€ freeway/
â”‚       â”‚   â””â”€â”€ train/
â”‚       â”‚       â””â”€â”€ <video_folder>/
â”‚       â”‚           â”œâ”€â”€ 00001.jpg
â”‚       â”‚           â”œâ”€â”€ 00002.jpg
â”‚       â”‚           â””â”€â”€ ...
â”‚       â””â”€â”€ road/
â”‚           â””â”€â”€ train/
â”‚               â””â”€â”€ <video_folder>/
â”‚                   â”œâ”€â”€ 00001.jpg
â”‚                   â”œâ”€â”€ 00002.jpg
â”‚                   â””â”€â”€ ...
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_loader.py    # Model loading logic
â”‚   â””â”€â”€ clip_fusion_net.py # Model architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py          # Config loading
â”‚   â”œâ”€â”€ setup.py           # Training setup utilities
â”‚   â”œâ”€â”€ log.py             # Logging helpers
â”‚   â”œâ”€â”€ training.py        # Training loop and helpers
â”‚   â”œâ”€â”€ board.py           # TensorBoard utilities
â”‚   â”œâ”€â”€ seed.py            # Random seed setup
â”‚   â”œâ”€â”€ extract_bb.py      # YOLO vehicle detection
â”‚   â””â”€â”€ optical_flow.py    # Optical flow feature extraction
â”œâ”€â”€ runs/                  # Training outputs (created automatically)
â”‚   â””â”€â”€ YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ best_model.pth
â”‚       â”œâ”€â”€ log.txt
â”‚       â””â”€â”€ tensorboard/
â””â”€â”€ requirements.txt       # Python dependencies
```

</details>

---

## ðŸ”— References

- [Ultralytics YOLO Documentation](https://docs.ultralytics.com/)
- [CLIP Model](https://github.com/openai/CLIP)
- [TAISC 2025](https://sites.google.com/view/avss2025-tw/taisc)

---

## ðŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.





